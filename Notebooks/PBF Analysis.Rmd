---
output: html_document
--- 
## **Philaldelphia Bail Fund** 

\right<span style="color: grey;">July 23, 2020</span>\right  

  [GitHub](https://github.com/CodeForPhilly/pbf-analysis)  
  [Website](https://www.phillybailfund.org/)
  

***

#### _**Table of Contents**_
* [Introduction](#**Introduction**) 
* [Data Cleaning](#**Data Cleaning**) 
* [Text Preprocessing](#**Text Preprocessing**) 
* [Predictive Modeling](#**Predictive Modeling**) 
* [Analysis](#**Analysis**) 
* [Ideas for Further Analysis](#**Ideas for Further Analysis**)

***

## <a id="**Introduction**"></a>**Introduction**
[]

## <a id="**Data Cleaning**"></a>**Data Cleaning**
```{r message = FALSE, warning = FALSE}
library(dplyr)
library(corrplot)
library(Metrics)
library(gbm)
library(ggplot2)
library(stringr)
library(tm) 
library(textstem)
library(tidytext)
library(wordcloud)
```
##### A1. Import data
```{r}
url <- "https://raw.githubusercontent.com/CodeForPhilly/pbf-analysis/master/Data/0c_distinct_dockets.csv"
data <- read.csv(url)
head(data, 10)
```

##### A2. Delete rows where bail_type = 'Denied' and bail_status = "ROR" or "Nonmonetary" 
```{r}
data <- data %>% mutate(bail_status = as.character(bail_status),
                        bail_type = as.character(bail_type)) %>% 
  filter(!bail_status %in% c("Nonmonetary", "ROR"), 
         !bail_type %in% c("Denied"))
```

##### A3. Create hour of day and day of week from filing_date
```{r}
data$date <- as.Date(data$filing_date)
data$day_of_week <- as.factor(weekdays(data$date))
data$time <- as.numeric(substr(data$filing_date, 12, 13))
data$bail_status <- as.factor(data$bail_status)
```

###### A4. Impute age with average of age
```{r}
avg.age <- mean(data[-which(is.na(data)),]$age)
data <- data %>% mutate(age = ifelse(is.na(age) == TRUE, avg.age, age))

ggplot(data, aes(x=date, y=bail_amount)) +
  geom_line( color="steelblue") + 
  xlab("")
```

##### A5. Keep = bail_amount, charge, day of week, hour of day, age, represented_by, address

  

  

## <a id="**Text Preprocessing**"></a>**Text Preprocessing**
##### B1. Clean text
```{r}
data$charge = tolower(data$charge)

new_stopwords <- c(stopwords('en'), "criminal", "crim", "cause", "intent", "int", "attempt")
data$charge = gsub(paste(new_stopwords, collapse = '\\b|\\b'), '', data$charge)
data$charge = gsub('[[:digit:]]', '', data$charge)
data$charge = gsub('[[:punct:]]', '', data$charge)
data$charge = gsub('\\s+',' ', data$charge)
```

##### B2. Group charges - reduce from 136 to 97 levels
```{r}
#data$charge <- as.character(data$charge)
data$charge <- ifelse(str_detect(data$charge, "burglary"), "burglary",
                      ifelse(str_detect(data$charge, "robbery"), "robbery", 
                             ifelse(str_detect(data$charge, "terrorize"), "terrorist",
                                    ifelse(str_detect(data$charge, "murder"), "murder", 
                                           ifelse(str_detect(data$charge, "statutory"), "statutory sexual assault",
                                                  ifelse(str_detect(data$charge, "harassment"), "harassment",
                                                         ifelse(str_detect(data$charge, "agg"), "aggravated assault",
                                                                ifelse(str_detect(data$charge, "theft"), "theft",
                                                                       ifelse(str_detect(data$charge, "asslt"), "indecent assault",
                                                                              ifelse(str_detect(data$charge, "firearm"), "firearm possession/delivery",
                                                                                     ifelse(str_detect(data$charge, "rape"), "rape",
                                                                                    data$charge)))))))))))

#data$charge = lemmatize_strings(data$charge)
data$charge <- as.factor(data$charge)
str(data)
```

##### B3. Create tdm
```{r}
corp <- Corpus(VectorSource(data$charge))
tdm <- TermDocumentMatrix(corp, control = list(wordLengths = c(1, Inf)))
tidy_frame <- tidy(tdm)
```

##### D7. Wordcloud
```{r}
cloud_data <- tidy_frame %>% group_by(term) %>% summarise(counts = sum(count))
head(cloud_data %>% arrange(-counts))  # these are the most common words
wordcloud(words=cloud_data$term, freq=cloud_data$counts, random.order=FALSE, colors=brewer.pal(7, "Greens"), max.words = 20)
```

## <a id="**Predictive Modeling**"></a>**Analysis**
###########################################
#               Data Modeling             #
###########################################

C1
```{r}
data.gb <- data[, c(2, 6, 7, 10, 13, 14)] 

set.seed(123) 
train_index <- sample(nrow(data.gb), size = round(0.75 * nrow(data.gb)), replace = FALSE)
train <- data.gb[train_index,]
test <- data.gb[-train_index,]
```


C2
```{r}
set.seed(123)
gb_parameters <- data.frame(sample_size = round(runif(10,0.1,0.6), 1),
                            shrink = round(runif(10,0.001,0.008), 3))

mape_gb <- data.frame(mape_train_gb = numeric(), mape_test_gb = numeric())
#rmse_gb <- data.frame(rmse_train_gb = numeric(), rmse_test_gb = numeric())

for(paracomb_gb in 1:nrow(gb_parameters)){
  gradient_boosting <- gbm(bail_amount ~ ., data = train, 
                           distribution = "gaussian",
                           n.trees = 500,
                           interaction.depth = 6,
                           n.minobsinnode = 5, 
                           cv.folds = 3,
                           bag.fraction = gb_parameters[paracomb_gb,'sample_size'], 
                           shrinkage = gb_parameters[paracomb_gb,'shrink']
                           )
  
  pred_train_gb <- predict(gradient_boosting, train, n.trees = 500)
  mape_train_gb <- mape(pred_train_gb, train$bail_amount)#
  #rmse(train$bail_amount, pred_train_gb)
  
  pred_test_gb <- predict(gradient_boosting, test, n.trees = 500)
  mape_test_gb <- mape(pred_test_gb, test$bail_amount)#
  #rmse(train$bail_amount, pred_test_gb)
  
  mape_gb[paracomb_gb, ] <- c(mape_train_gb, mape_test_gb)
  #rmse_gb[paracomb_gb, ] <- c(rmse_train_gb, rmse_test_gb)
  
}

cbind(gb_parameters, mape_gb)

set.seed(123)
best_gradient_boosting <- gbm(bail_amount ~ ., data = train, 
                              distribution = "gaussian",
                              n.trees = 500,
                              interaction.depth = 6,
                              n.minobsinnode = 5, 
                              cv.folds = 3,
                              bag.fraction = 0.4 , 
                              shrinkage = 0.003
)

gbm.perf(best_gradient_boosting, method = "cv")

pred_train_bestgb <- predict(best_gradient_boosting, train, n.trees = 500)
mape(pred_train_bestgb, train$bail_amount)

pred_test_bestgb <- predict(best_gradient_boosting, test, n.trees = 500)
mape(pred_test_bestgb, test$bail_amount)

summary(best_gradient_boosting)
explain_gb <- explain(model = best_gradient_boosting, data = train,
                      label = "Generalized Boosted Regression", verbose = FALSE)

x_ax = 1:length(pred_test_bestgb)
plot(x_ax, pred_test_bestgb, col="blue", pch=20, cex=.9)
lines(x_ax, pred_test_bestgb, col="red", pch=20, cex=.9) 

par.Petal_W <- partial(best_gradient_boosting, pred.var = "age", n.trees = 500, plot = TRUE)
plot(best_gradient_boosting, i = "age")
```



## <a id="**Analysis**"></a>**Analysis**
 
```{r fig.align="center"}
correlation <- cor(data.gb[,c(1,6,8)])
corrplot(correlation, type = "lower", col = c("pink", "lightblue"), addCoef.col = "black", tl.col = "black")
```  

