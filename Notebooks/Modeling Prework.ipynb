{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philly Bail Fund\n",
    "## Modeling Prework necessary for analysis\n",
    "\n",
    "This notebook is intended to train and export the models necessary to perform analysis in another notebook. For more details, see the github repo: https://github.com/CodeForPhilly/pbf-analysis\n",
    "\n",
    "[insert overview of approach here.. img?]\n",
    "\n",
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard Imports - Sorry PEP8 fans, do not look below\n",
    "import pandas as pd, numpy as np, os, re, json, pickle, math, calendar\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "## Specific Imports\n",
    "import hashlib\n",
    "\n",
    "# frameworks\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "# feat engineering\n",
    "import category_encoders as ce\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# modeling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# validation and scoring\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "\n",
    "### Display options for notebooks\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 25)\n",
    "\n",
    "### set path directories\n",
    "curr_dir = Path(os.getcwd())\n",
    "#print('Current Directory is: ', str(curr_dir))\n",
    "data_dir = Path(curr_dir.parents[0] / 'Data/')\n",
    "artifacts_dir = Path(curr_dir / 'artifacts/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Common project specific variables\n",
    "FILENAME = '0c_distinct_dockets.csv'  # original data\n",
    "TARGET_VARIABLE_NAME = 'bail_amount'\n",
    "HOLDOUT_INDICATOR_NAME = 'holdout_ind'\n",
    "HOLDOUT_SIZE = 0.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to reduce memory footprint of the dataframe\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    import numpy as np\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} MB ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import\n",
    "\n",
    "Also, check a hash to see if this file has changed since this code was written. If it changes, someone should review this notebook to make sure that the code still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimental - looking to find a way to prevent this notebook from continuing\n",
    "# if by chance the dataset has changed\n",
    "BLOCKSIZE = 65536\n",
    "hasher = hashlib.md5()\n",
    "with open(Path(data_dir) / FILENAME, 'rb') as afile:\n",
    "    buf = afile.read(BLOCKSIZE)\n",
    "    while len(buf) > 0:\n",
    "        hasher.update(buf)\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        \n",
    "filehash = hasher.hexdigest()\n",
    "\n",
    "with open(Path(artifacts_dir) / 'data_file_hash.txt', 'rb') as f:\n",
    "    checkhash = pickle.load(f)\n",
    "    \n",
    "if filehash != checkhash:\n",
    "    print(\"!! Warning !! The file is different than when this code was last updated. \\nProceed with caution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a bootleg way to update the hash manually, if we know everything is good to go\n",
    "UPDATE_HASH_FLAG = False\n",
    "\n",
    "if UPDATE_HASH_FLAG==True:\n",
    "    with open(Path(artifacts_dir) / 'data_file_hash.txt', 'wb') as f:\n",
    "        pickle.dump(filehash, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>docket_number</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>charge</th>\n",
       "      <th>represented_by</th>\n",
       "      <th>bail_type</th>\n",
       "      <th>bail_status</th>\n",
       "      <th>bail_amount</th>\n",
       "      <th>outstanding_bail_amount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3909</th>\n",
       "      <td>27.0</td>\n",
       "      <td>Philadelphia, PA 19141</td>\n",
       "      <td>MC-51-CR-0011746-2020</td>\n",
       "      <td>2020-06-16 00:37:00+00:00</td>\n",
       "      <td>DUI: Gen Imp/Inc of Driving Safely - 1st Off</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Posted</td>\n",
       "      <td>ROR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>44.0</td>\n",
       "      <td>Philadelphia, PA 19124</td>\n",
       "      <td>MC-51-CR-0011747-2020</td>\n",
       "      <td>2020-06-16 00:41:00+00:00</td>\n",
       "      <td>Verify Address or Photographed as Required</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Set</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>24.0</td>\n",
       "      <td>Philadelphia, PA 19142</td>\n",
       "      <td>MC-51-CR-0011743-2020</td>\n",
       "      <td>2020-06-16 00:52:00+00:00</td>\n",
       "      <td>Criminal Mischief</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Posted</td>\n",
       "      <td>ROR</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age                 address          docket_number  \\\n",
       "id                                                          \n",
       "3909  27.0  Philadelphia, PA 19141  MC-51-CR-0011746-2020   \n",
       "4538  44.0  Philadelphia, PA 19124  MC-51-CR-0011747-2020   \n",
       "120   24.0  Philadelphia, PA 19142  MC-51-CR-0011743-2020   \n",
       "\n",
       "                   filing_date                                        charge  \\\n",
       "id                                                                             \n",
       "3909 2020-06-16 00:37:00+00:00  DUI: Gen Imp/Inc of Driving Safely - 1st Off   \n",
       "4538 2020-06-16 00:41:00+00:00    Verify Address or Photographed as Required   \n",
       "120  2020-06-16 00:52:00+00:00                             Criminal Mischief   \n",
       "\n",
       "                            represented_by bail_type bail_status  bail_amount  \\\n",
       "id                                                                              \n",
       "3909  Defender Association of Philadelphia    Posted         ROR            0   \n",
       "4538  Defender Association of Philadelphia       Set    Monetary        50000   \n",
       "120   Defender Association of Philadelphia    Posted         ROR            0   \n",
       "\n",
       "      outstanding_bail_amount  \n",
       "id                             \n",
       "3909                        0  \n",
       "4538                        0  \n",
       "120                         0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data and take a peek at it\n",
    "indata = pd.read_csv(Path(data_dir) / FILENAME, parse_dates=['filing_date'], index_col='id')\n",
    "\n",
    "indata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup\n",
    "\n",
    "A1. Keep = bail_amount, charge, bail_status, filing_date, age, represented_by\n",
    "\n",
    "A2. Create hour of day and day of week from filing_date, then drop originial filing_date\n",
    "\n",
    "A3. Delete rows where bail_status = 'Denied' (we will only worry about ones where there is a set amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A1: Keep only columns that might impact the bail amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop things we know aren't useful\n",
    "drop_list = ['address','docket_number','bail_type','outstanding_bail_amount']\n",
    "\n",
    "indata.drop(columns=drop_list, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A2: Parse Hour of Day and Day of Week, before dropping the date field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datecol = 'filing_date'\n",
    "\n",
    "# hour of day\n",
    "indata['filed_hour_of_day'] = indata[datecol].dt.hour\n",
    "\n",
    "# day of the week with Monday=0, Sunday=6\n",
    "indata['filed_day_of_week'] = indata[datecol].dt.dayofweek.apply(lambda x: calendar.day_abbr[x])\n",
    "\n",
    "indata.drop(columns=[datecol], inplace=True, errors='ignore')\n",
    "\n",
    "indata['charge'] = indata['charge'].str.lower().str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A3: Remove rows where bail does not apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>charge</th>\n",
       "      <th>represented_by</th>\n",
       "      <th>bail_status</th>\n",
       "      <th>bail_amount</th>\n",
       "      <th>filed_hour_of_day</th>\n",
       "      <th>filed_day_of_week</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>44.0</td>\n",
       "      <td>verify address or photographed as required</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>50000</td>\n",
       "      <td>0</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>32.0</td>\n",
       "      <td>contempt for violation of order or agreement</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>32.0</td>\n",
       "      <td>burglary  overnight accommodations person pres...</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>75000</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>32.0</td>\n",
       "      <td>burglary  overnight accommodations person pres...</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>75000</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>51.0</td>\n",
       "      <td>simple assault</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>25000</td>\n",
       "      <td>1</td>\n",
       "      <td>Tue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age                                             charge  \\\n",
       "id                                                              \n",
       "4538  44.0         verify address or photographed as required   \n",
       "291   32.0       contempt for violation of order or agreement   \n",
       "291   32.0  burglary  overnight accommodations person pres...   \n",
       "291   32.0  burglary  overnight accommodations person pres...   \n",
       "2396  51.0                                     simple assault   \n",
       "\n",
       "                            represented_by bail_status  bail_amount  \\\n",
       "id                                                                    \n",
       "4538  Defender Association of Philadelphia    Monetary        50000   \n",
       "291   Defender Association of Philadelphia    Monetary        50000   \n",
       "291   Defender Association of Philadelphia    Monetary        75000   \n",
       "291   Defender Association of Philadelphia    Monetary        75000   \n",
       "2396  Defender Association of Philadelphia   Unsecured        25000   \n",
       "\n",
       "      filed_hour_of_day filed_day_of_week  \n",
       "id                                         \n",
       "4538                  0               Tue  \n",
       "291                   1               Tue  \n",
       "291                   1               Tue  \n",
       "291                   1               Tue  \n",
       "2396                  1               Tue  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove rows where bail amount is more than zero\n",
    "clean = indata[indata['bail_amount']>0]\n",
    "\n",
    "clean.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into Train and holdout\n",
    "\n",
    "In case we want to do special encoding that uses target signals, we want to ensure we do this now. But, it means we'll have to remember to apply the transformations to the test dataset as well (more coding, blah)\n",
    "\n",
    "So that I can compare this method with other software and techniques, I'm adding an indicator for the holdout and each of the 5 training folds, so that we can replicate results and compare directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>charge</th>\n",
       "      <th>represented_by</th>\n",
       "      <th>bail_status</th>\n",
       "      <th>bail_amount</th>\n",
       "      <th>filed_hour_of_day</th>\n",
       "      <th>filed_day_of_week</th>\n",
       "      <th>holdout_ind</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>38.0</td>\n",
       "      <td>manufacture delivery or possession with intent...</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>350000</td>\n",
       "      <td>4</td>\n",
       "      <td>Wed</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>34.0</td>\n",
       "      <td>terroristic threats w int to terrorize another</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>25000</td>\n",
       "      <td>19</td>\n",
       "      <td>Sun</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>29.0</td>\n",
       "      <td>burglary  not adapted for overnight accommodat...</td>\n",
       "      <td>None</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age                                             charge  \\\n",
       "id                                                              \n",
       "4161  38.0  manufacture delivery or possession with intent...   \n",
       "1543  34.0     terroristic threats w int to terrorize another   \n",
       "2377  29.0  burglary  not adapted for overnight accommodat...   \n",
       "\n",
       "                            represented_by bail_status  bail_amount  \\\n",
       "id                                                                    \n",
       "4161  Defender Association of Philadelphia    Monetary       350000   \n",
       "1543  Defender Association of Philadelphia    Monetary        25000   \n",
       "2377                                  None   Unsecured        50000   \n",
       "\n",
       "      filed_hour_of_day filed_day_of_week holdout_ind  \n",
       "id                                                     \n",
       "4161                  4               Wed          T1  \n",
       "1543                 19               Sun          T1  \n",
       "2377                  1               Mon          T1  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a handy little routine that will add a partition indicator including holdout and folds\n",
    "# this is handy for benchmarking in another tool where you want the exact same fold partitions\n",
    "# and cant trust the random seed\n",
    "\n",
    "with pd.option_context('mode.chained_assignment', None):\n",
    "    temptrain, holdoutdata = train_test_split(\n",
    "        clean,\n",
    "        test_size=(1 - HOLDOUT_SIZE),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    kf = KFold(n_splits=5)\n",
    "    i = 0\n",
    "    for _ , test_index in kf.split(temptrain,temptrain[TARGET_VARIABLE_NAME]):\n",
    "        i+=1\n",
    "        temp = temptrain.iloc[test_index]\n",
    "        temp[HOLDOUT_INDICATOR_NAME]='T' + str(i)\n",
    "\n",
    "        if i==1:\n",
    "            traindata = temp.copy()\n",
    "        else:\n",
    "            traindata = traindata.append(temp)\n",
    "\n",
    "    holdoutdata[HOLDOUT_INDICATOR_NAME]='H'\n",
    "    clean = pd.concat([traindata,holdoutdata])\n",
    "\n",
    "# peek at the data\n",
    "clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this data so we can benchmark w/ AutoML tools if we want to\n",
    "clean.to_csv(Path(artifacts_dir) / 'ready_for_external_tests.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline including Feature Engineering\n",
    "B1. Encode categorical variables (bail_status, hour of day, day of week, represented_by, even charge!) \n",
    "using categorical_encoders library. Choose any method but best is probably Ordinal. Also \n",
    "good to try is just using alphabetical ordering and numbering them 1,2,3.. etc\n",
    "\n",
    "B2. Impute numeric variable (age) with -9999\n",
    "\n",
    "B3. Vectorize and process `charges` as text with NLP techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split holdout and training data\n",
    "train = clean[clean['holdout_ind'] != 'H']\n",
    "holdout = clean[clean['holdout_ind'] == 'H']\n",
    "\n",
    "# variables that need encoded or treated in special ways\n",
    "vars_numeric = ['age']\n",
    "vars_ordinal = ['represented_by','bail_status','filed_hour_of_day','filed_day_of_week']\n",
    "vars_txt = 'charge'\n",
    "\n",
    "# vocab approach for limiting the word vectorizer\n",
    "vocab = ['1st','1st off','agreement','arrest prior','assault','assault by','attempt','attempt burglary','attempt murder','attempt rape','attempt theft','by vehicle','child','compulsion','contempt','contempt for',\n",
    "'crime','crime int','criminal','criminal attempt','criminal mischief','degree','delivery or','driving','driving safely','dui','dui gen','enforcement','enforcement officer','escape','firearm','firearm prohibited',\n",
    "'first','first degree','for','for violation','forcible','forcible compulsion','from','from motor','gen','gen imp','imp','imp inc','inc','inc of','inflict serious','inj','instrument','instrument of','int',\n",
    "'involuntary','involuntary manslaughter','law','law enforcement','manslaughter','mischief','motor vehicle','murder','murder of','of','of crime','of driving','of firearm','of law','of order','of the','off',\n",
    "'officer','officer of','order','order or','other','per','person','person present','poss','poss instrument','possession','possession of','prior','prior to','prohibited','rape','rape forcible','requisition',\n",
    "'safely','simple','simple assault','the','the first','theft','theft from','to requisition','tres','vehicle while','violation','while','while dui']\n",
    "\n",
    "\n",
    "## it is really important that we export these columns in the correct order\n",
    "# This makes it easier to analyze in the analysis notebook\n",
    "# Pipelines are horrible at tracking feature names, see this post for the \n",
    "# workaround that didn't even work! \n",
    "# https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api/57534118#57534118\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "preprocessor_col_order = unique(vars_numeric + vars_ordinal + list(train.columns))\n",
    "\n",
    "preprocessor_col_order.remove(HOLDOUT_INDICATOR_NAME)\n",
    "\n",
    "with open(Path(artifacts_dir) / 'preprocessor_col_order.list', 'wb') as f:\n",
    "    joblib.dump(preprocessor_col_order, f, compress=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model only on text features\n",
    "\n",
    "We'll first build a model using the text features, before feeding that prediction into a final model where the txt is now represented as a single column (probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Text-Only Model) Grid Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "done in 65.306s\n",
      "\n",
      "Best score: -157429.819\n",
      "Best parameters set:\n",
      "'\\nsubmodel__alpha: 0.0001'\n",
      "'\\nsubmodel__max_iter: 5000'\n",
      "\"\\nsubmodel__penalty: 'l2'\"\n",
      "'\\nsubmodel__tol: 0.0005'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh.berry/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1211: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#### text preprocessor\n",
    "# note the .fit() is required for text processing\n",
    "\n",
    "pipeline_txt = Pipeline(steps=[('vect_counts', CountVectorizer(vocabulary=vocab, lowercase=True, ngram_range=(1,3), analyzer='word'))\n",
    "                               ,('vect_tfidf', TfidfTransformer())\n",
    "                              ])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('txt', pipeline_txt.fit(train[vars_txt]), vars_txt)\n",
    "                 ])\n",
    "\n",
    "# initiate a model\n",
    "submodel = SGDRegressor()\n",
    "\n",
    "# dimensionality reduction\n",
    "svd = TruncatedSVD()\n",
    "\n",
    "# option A: One combined pipeline                  \n",
    "#textpipe = Pipeline(steps=[('preprocessor', preprocessor),('svd', svd), ('submodel', submodel)])\n",
    "\n",
    "# option B: Do it without svd (by using vocab instead)\n",
    "textpipe_nosvd = Pipeline(steps=[('preprocessor', preprocessor), ('submodel', submodel)])\n",
    "\n",
    "# grid search settings\n",
    "params = {\n",
    "    'submodel__tol': (0, 0.0001, 0.0005, 0.001),\n",
    "    'submodel__alpha': (0, 0.0001, 0.0005, 0.001),\n",
    "    'submodel__penalty': ('l2', 'elasticnet'),\n",
    "    'submodel__max_iter': (500, 1000, 2000, 5000)\n",
    "}\n",
    "\n",
    "# initialize a cross fold validation strategy\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected block\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize a grid search of the pipeline and parameters \n",
    "    gs = GridSearchCV(textpipe_nosvd, param_grid=params, cv = cv, refit=True, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    # grid search (go get coffee)\n",
    "    print(\"Performing grid search...\")\n",
    "    xvars = list([vars_txt] + [HOLDOUT_INDICATOR_NAME])\n",
    "    gs.fit(train[xvars], train[TARGET_VARIABLE_NAME], groups=train[HOLDOUT_INDICATOR_NAME])\n",
    "    \n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    \n",
    "    # print the best settings\n",
    "    best_parameters = gs.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(params.keys()):\n",
    "        pprint(\"\\n%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chosen Text Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh.berry/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_stochastic_gradient.py:1211: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# text vectorizer\n",
    "pipeline_txt = Pipeline(steps=[('vect_counts', CountVectorizer(vocabulary=vocab, lowercase=True, ngram_range=(1,3), analyzer='word'))\n",
    "                              ,('vect_tfidf', TfidfTransformer())\n",
    "                              ])\n",
    "\n",
    "# text preprocessor\n",
    "text_preprocessor = ColumnTransformer(\n",
    "    transformers=[('txt', pipeline_txt.fit(train[vars_txt]), vars_txt)])\n",
    "\n",
    "# text dimensionality reduction\n",
    "#text_svd = TruncatedSVD(n_components=100, n_iter=5)\n",
    "\n",
    "# Final submodel for text features\n",
    "text_submodel = SGDRegressor(max_iter=5000, alpha=0.0001, penalty='l2',tol=0.0005)\n",
    "\n",
    "# hardcode the final pipeline\n",
    "text_finalpipe = Pipeline(steps=[('text_preprocessor', text_preprocessor), ('text_submodel',text_submodel)])\n",
    "\n",
    "text_model = text_finalpipe.fit(train[[vars_txt]], train[TARGET_VARIABLE_NAME])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later\n",
    "with open(Path(artifacts_dir) / 'text_submodel.mdl', 'wb') as f:\n",
    "    joblib.dump(text_model, f, compress = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intermediate Scoring to replace 'charge' with numeric representation\n",
    "\n",
    "I should be able to figure out how to include this in the pipeline later?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh.berry/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/josh.berry/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>charge</th>\n",
       "      <th>represented_by</th>\n",
       "      <th>bail_status</th>\n",
       "      <th>bail_amount</th>\n",
       "      <th>filed_hour_of_day</th>\n",
       "      <th>filed_day_of_week</th>\n",
       "      <th>holdout_ind</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>38.0</td>\n",
       "      <td>85183.001412</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>350000</td>\n",
       "      <td>4</td>\n",
       "      <td>Wed</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>34.0</td>\n",
       "      <td>19397.061573</td>\n",
       "      <td>Defender Association of Philadelphia</td>\n",
       "      <td>Monetary</td>\n",
       "      <td>25000</td>\n",
       "      <td>19</td>\n",
       "      <td>Sun</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>29.0</td>\n",
       "      <td>22440.811352</td>\n",
       "      <td>None</td>\n",
       "      <td>Unsecured</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>Mon</td>\n",
       "      <td>T1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age        charge                        represented_by bail_status  \\\n",
       "id                                                                           \n",
       "4161  38.0  85183.001412  Defender Association of Philadelphia    Monetary   \n",
       "1543  34.0  19397.061573  Defender Association of Philadelphia    Monetary   \n",
       "2377  29.0  22440.811352                                  None   Unsecured   \n",
       "\n",
       "      bail_amount  filed_hour_of_day filed_day_of_week holdout_ind  \n",
       "id                                                                  \n",
       "4161       350000                  4               Wed          T1  \n",
       "1543        25000                 19               Sun          T1  \n",
       "2377        50000                  1               Mon          T1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this entire block could be written better\n",
    "train['charge'] = text_model.predict(train[[vars_txt]])\n",
    "holdout['charge'] = text_model.predict(holdout[[vars_txt]])\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Main Model) Grid Search for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "done in 871.350s\n",
      "\n",
      "Best score: -148061.184\n",
      "Best parameters set:\n",
      "'\\nregression__max_depth: None'\n",
      "'\\nregression__max_features: 0.6'\n",
      "'\\nregression__min_impurity_decrease: 0.1'\n",
      "'\\nregression__min_samples_leaf: 2'\n",
      "'\\nregression__min_samples_split: 10'\n",
      "'\\nregression__n_estimators: 400'\n"
     ]
    }
   ],
   "source": [
    "#### Basic Pipelines\n",
    "# impute missing for numeric columns\n",
    "pipeline_numeric = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=-9999))])\n",
    "\n",
    "# ordinal encoding\n",
    "pipeline_ordinal = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                          ('ord_encoding', ce.ordinal.OrdinalEncoder())])\n",
    "# target encoding\n",
    "#pipeline_tar = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),('tar_encoding', ce.SumEncoder())])\n",
    "\n",
    "# TODO: fit text into a submodel as part of pipeline\n",
    "\n",
    "#### one unified preprocessor\n",
    "# note the .fit() is required for text processing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', pipeline_numeric, vars_numeric),\n",
    "                  ('cat1', pipeline_ordinal, vars_ordinal)\n",
    "                  #('cat2', pipeline_tar, vars_tar)\n",
    "                 ], remainder='passthrough')\n",
    "\n",
    "# initiate a model\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# name the final pipeline connected to the model                    \n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor), ('regression', rfr)])\n",
    "\n",
    "# parameter options for grid search\n",
    "params = {\"regression__max_depth\": [2, 4, 6, None],\n",
    "              \"regression__max_features\": [0.6, 0.7],\n",
    "              \"regression__min_samples_split\": [10, 20],\n",
    "              \"regression__min_samples_leaf\": [2, 5, 10],\n",
    "              \"regression__n_estimators\": [100, 200, 400, 500],\n",
    "              \"regression__min_impurity_decrease\": [0.001, 0.01, 0.1, 0.2, 0.5]}\n",
    "\n",
    "# initialize a cross fold validation strategy\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected block\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize a grid search of the pipeline and parameters \n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv = cv, refit=True, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    t0 = time()\n",
    "    \n",
    "    # grid search (go get coffee)\n",
    "    print('Performing grid search...')\n",
    "    \n",
    "    X = train.drop([TARGET_VARIABLE_NAME, HOLDOUT_INDICATOR_NAME], axis=1)\n",
    "    y = train[TARGET_VARIABLE_NAME]\n",
    "    \n",
    "    gs.fit(X, y, groups=train[HOLDOUT_INDICATOR_NAME])\n",
    "    \n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % gs.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    \n",
    "    # print the best settings\n",
    "    best_parameters = gs.best_estimator_.get_params()\n",
    "    \n",
    "    for param_name in sorted(params.keys()):\n",
    "        pprint(\"\\n%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D1: Find the best Random Forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Performance of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic Pipelines\n",
    "####### Remember that this ordering matters!! See up above where the vars_* variables are set\n",
    "\n",
    "# impute missing for numeric columns\n",
    "pipeline_numeric = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value=-9999))])\n",
    "\n",
    "# ordinal encoding\n",
    "pipeline_ordinal = Pipeline(steps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                                          ('ord_encoding', ce.ordinal.OrdinalEncoder())])\n",
    "\n",
    "# hardcode the model to the best settings\n",
    "best_model_settings = RandomForestRegressor(n_estimators=400\n",
    "                                            ,criterion='mse'\n",
    "                                            ,max_depth=None\n",
    "                                            ,min_samples_split=10\n",
    "                                            ,min_samples_leaf=2\n",
    "                                            ,min_weight_fraction_leaf=0.0\n",
    "                                            ,max_features=0.6\n",
    "                                            ,max_leaf_nodes=1000\n",
    "                                            ,min_impurity_decrease=0.1\n",
    "                                            ,bootstrap=False\n",
    "                                            ,oob_score=False\n",
    "                                            ,n_jobs=None\n",
    "                                            ,random_state=7\n",
    "                                            ,verbose=0\n",
    "                                            ,warm_start=False\n",
    "                                            ,ccp_alpha=0.0\n",
    "                                            ,max_samples=None)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', pipeline_numeric, vars_numeric),\n",
    "                  ('cat1', pipeline_ordinal, vars_ordinal)\n",
    "                 ], remainder='passthrough')\n",
    "\n",
    "\n",
    "# hardcode the final pipeline\n",
    "final_pipe = Pipeline(steps=[('preprocessor', preprocessor),('final_model', best_model_settings)])\n",
    "\n",
    "# export the column names for correct ordering\n",
    "cols = train.drop([TARGET_VARIABLE_NAME, HOLDOUT_INDICATOR_NAME], axis=1).columns\n",
    "\n",
    "with open(Path(artifacts_dir) / 'model_col_order.list', 'wb') as f:\n",
    "    joblib.dump(cols, f, compress=0)\n",
    "    \n",
    "# fit the model on the training data (remember this is in-sample and scores will look overfit)\n",
    "model = final_pipe.fit(train[cols], train[TARGET_VARIABLE_NAME])\n",
    "\n",
    "# predictions\n",
    "train_y_pred = model.predict(train.drop([TARGET_VARIABLE_NAME, HOLDOUT_INDICATOR_NAME], axis=1))\n",
    "holdout_y_pred = model.predict(holdout.drop([TARGET_VARIABLE_NAME, HOLDOUT_INDICATOR_NAME], axis=1))\n",
    "\n",
    "# naive guess whatever the average of the training data was\n",
    "naive_guess = pd.Series(np.mean(((train[TARGET_VARIABLE_NAME]) * len(train)) + (np.mean(holdout[TARGET_VARIABLE_NAME]) * len(holdout))) / (len(train) + len(holdout)))\n",
    "naive_y_pred = naive_guess.repeat(len(holdout_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Error listed as RMSE ******\n",
      "Train (In-Sample, cannot trust this): 99059\n",
      "CV: 148061.0\n",
      "Holdout: 145197\n",
      "Naive: 186698\n"
     ]
    }
   ],
   "source": [
    "# calc the RMSE\n",
    "train_mse = mean_squared_error(train[TARGET_VARIABLE_NAME], train_y_pred)\n",
    "holdout_mse = mean_squared_error(holdout[TARGET_VARIABLE_NAME], holdout_y_pred)\n",
    "naive_mse = mean_squared_error(holdout[TARGET_VARIABLE_NAME], naive_y_pred)\n",
    "\n",
    "print('***** Error listed as RMSE ******')\n",
    "print('Train (In-Sample, cannot trust this): ' + str(round(math.sqrt(train_mse))))\n",
    "try:\n",
    "    print('CV: ' + str(-round(gs.best_score_)))\n",
    "except:\n",
    "    pass\n",
    "print('Holdout: ' + str(round(math.sqrt(holdout_mse))))\n",
    "print('Naive: ' + str(round(math.sqrt(naive_mse))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have small data, but at least we have somewhat of a model that is slightly better than average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it for later\n",
    "with open(Path(artifacts_dir) / 'model.mdl', 'wb') as f:\n",
    "    joblib.dump(model, f, compress=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
